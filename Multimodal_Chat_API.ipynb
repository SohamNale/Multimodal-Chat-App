{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U flask-cors\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Log in to Hugging Face (ensure you have your token)\n",
        "login(\"Use your hugginface token\")\n",
        "\n",
        "# Set up device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the Llama 3.2 model\n",
        "llama_32 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "generator_1 = pipeline(model=llama_32, device=device, torch_dtype=torch.bfloat16)\n",
        "\n",
        "mistral_3b = \"ministral/Ministral-3b-instruct\"\n",
        "generator_2 = pipeline(model=mistral_3b, device=device, torch_dtype=torch.bfloat16)\n",
        "\n",
        "history = []\n",
        "\n",
        "# Define function for Llama 3.2 model\n",
        "def generate_llama_response(message):\n",
        "    context_prompt = \"You are a helpful assistant. Provide concise answers.\\n\\n\"\n",
        "\n",
        "    # Compile history into the context for the prompt\n",
        "    for messages in history:\n",
        "        role = \"User\" if messages[\"role\"] == \"user\" else \"Assistant\"\n",
        "        context_prompt += f\"{role}: {messages['content']}\\n\"\n",
        "\n",
        "    # Add the latest user message\n",
        "    context_prompt += f\"User: {message}\\nAssistant:\"\n",
        "\n",
        "    prompt = [\n",
        "        #{\"role\": \"system\", \"content\": context_prompt},\n",
        "        {\"role\": \"user\", \"content\": context_prompt},\n",
        "    ]\n",
        "\n",
        "    outputs = generator_1(\n",
        "        prompt,\n",
        "        do_sample=False,\n",
        "        temperature=1.0,\n",
        "        top_p=1,\n",
        "        max_new_tokens=500\n",
        "    )\n",
        "    x = outputs[0][\"generated_text\"][-1]\n",
        "    if x[\"role\"] == \"assistant\":\n",
        "        history.append({\"role\": \"user\", \"content\": message})\n",
        "        history.append({\"role\": \"assistant\", \"content\": x[\"content\"]})\n",
        "        return x[\"content\"]\n",
        "    return \"No valid response generated.\"\n",
        "\n",
        "\n",
        "# Define function for Gemini model\n",
        "def generate_gemini_response(message):\n",
        "    context_prompt = \"You are a helpful assistant. Provide concise answers.\\n\\n\"\n",
        "\n",
        "    # Compile history into the context for the prompt\n",
        "    for messages in history:\n",
        "        role = \"User\" if messages[\"role\"] == \"user\" else \"Assistant\"\n",
        "        context_prompt += f\"{role}: {messages['content']}\\n\"\n",
        "\n",
        "    # Add the latest user message\n",
        "    context_prompt += f\"User: {message}\\nAssistant:\"\n",
        "    # Replace 'Use your Gemini API Key' with your API Key\n",
        "    genai.configure(api_key=\"Use your Gemini API Key\")\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    response = model.generate_content(context_prompt)\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": response.text})\n",
        "    \n",
        "    return(response.text)\n",
        "\n",
        "\n",
        "# Define function for Mistral model\n",
        "def generate_mistral_response(message):\n",
        "    context_prompt = \"You are a helpful assistant. Provide concise answers.\\n\\n\"\n",
        "\n",
        "    # Compile history into the context for the prompt\n",
        "    for messages in history:\n",
        "        role = \"User\" if messages[\"role\"] == \"user\" else \"Assistant\"\n",
        "        context_prompt += f\"{role}: {messages['content']}\\n\"\n",
        "\n",
        "    # Add the latest user message\n",
        "    context_prompt += f\"User: {message}\\nAssistant:\"\n",
        "\n",
        "    prompt = [\n",
        "        #{\"role\": \"system\", \"content\": context_prompt},\n",
        "        {\"role\": \"user\", \"content\": context_prompt},\n",
        "    ]\n",
        "\n",
        "    outputs = generator_1(\n",
        "        prompt,\n",
        "        do_sample=False,\n",
        "        temperature=1.0,\n",
        "        top_p=1,\n",
        "        max_new_tokens=500\n",
        "    )\n",
        "    x = outputs[0][\"generated_text\"][-1]\n",
        "    if x[\"role\"] == \"assistant\":\n",
        "        history.append({\"role\": \"user\", \"content\": message})\n",
        "        history.append({\"role\": \"assistant\", \"content\": x[\"content\"]})\n",
        "        return x[\"content\"]\n",
        "    return \"No valid response generated.\"\n",
        "\n",
        "# Set up Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Route to handle requests\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    # Get user input from the POST request\n",
        "    data = request.json\n",
        "    message = data.get(\"message\", \"\")\n",
        "    model = data.get(\"model\", \"llama\")  # Default to Llama\n",
        "\n",
        "    if not message:\n",
        "        return jsonify({\"error\": \"No message provided.\"}), 400\n",
        "\n",
        "    try:\n",
        "        # Call the appropriate function based on the model\n",
        "        if model == \"llama\":\n",
        "            response = generate_llama_response(message)\n",
        "        elif model == \"gemini\":\n",
        "            response = generate_gemini_response(message)\n",
        "        elif model == \"mistral\":\n",
        "            response = generate_mistral_response(message)\n",
        "        else:\n",
        "            return jsonify({\"error\": f\"Model '{model}' not supported.\"}), 400\n",
        "\n",
        "        return jsonify({\"response\": response})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up ngrok to expose the Flask app\n",
        "    from pyngrok import ngrok\n",
        "\n",
        "    # Replace 'Use your ngrok token' with your ngrok authtoken\n",
        "    ngrok.set_auth_token(\"Use your ngrok token\")\n",
        "\n",
        "    # Start ngrok and get the public URL\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(port=5000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
